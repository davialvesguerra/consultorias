min_theta <- unlist(theta_est[which.min(custo_treino)])
round(min_theta, 3)
### Semente para reproducibilidade
set.seed(1.2024)
### Obtenção da população de previsões
A         <- matrix(rep(as.matrix(x_teste[1,]), 200), 200, byrow = T)
y_hat_200 <- forward_prop_drop(min_theta, A, p)
### Semente para reproducibilidade
set.seed(1.2024)
### Função para fazer a previsão de uma observação
pred_c <- function(x, theta, n, p) {
A <- matrix(rep(x, n), n, byrow = T)
return(mean(forward_prop_drop(theta, A, p)))
}
### Cálculo das previsões para todas as observações
y_hat_drop <- apply(x_teste, 1, pred_c, theta = min_theta, n = 200, p = p)
### Apresentação das primeiras 10 previsões
head(y_hat_drop, 10)
### Função para realizar o forward propagation, dados theta e x
forward_prop <- function(theta, x) {
## Transformação de x para um formato de matriz
ifelse(is.vector(x), x <- as.matrix(x), x <- t(as.matrix(x)))
## Extração dos parâmetros
W1 <- matrix(theta[1:4], 2)
W2 <- matrix(theta[5:6], 2)
b1 <- theta[7:8]
b2 <- theta[9]
## Camada escondida
a <- W1 %*% x + matrix(rep(b1, ncol(x)), 2)
h <- sigmoide(a)
## Previsão
y_hat <- as.double(t(W2) %*% h + b2)
return(y_hat)
}
### Estimativas usando weight scaling
theta_scale <- c(min_theta[1:6]*0.6, min_theta[7:9])
y_hat_scale <- forward_prop(theta_scale, x_teste)
### Custo usando o procedimento do item (c)
custo_c <- mse_cost(y_teste, y_hat_drop)
### Custo usando weight scaling
custo_scale <- mse_cost(y_teste, y_hat_scale)
### Comparação do tempo computacional utilizando microbenchmark
microbenchmark('Item c:' = y_hat_drop <- apply(x_teste, 1, pred_c,
theta = min_theta,
n = 200, p = p),
'Scale:'  = y_hat_scale <- forward_prop(theta_scale, x_teste),
times = 5)
### Carregamos o pacote keras
library(keras)
# install_keras()
### Iniciamos um modelo sequencial
mod <- keras_model_sequential()
install_keras()
### Carregamos o pacote keras
library(keras)
### Iniciamos um modelo sequencial
mod <- keras_model_sequential()
mod
### Adicionamos as camadas
mod %>%
layer_dense(units              = 2,
activation         = "sigmoid",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0),
input_shape        = ncol(x_treino)) %>%
layer_dense(units              = 1,
activation         = "linear",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0))
initializer_constant(0)
mod %>%
layer_dense(units              = 2,
activation         = "sigmoid",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0),
input_shape        = ncol(x_treino))
### Iniciamos um modelo sequencial
mod <- keras_model_sequential()
mod
mod %>%
layer_dense(units              = 2,
activation         = "sigmoid",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0),
input_shape        = ncol(x_treino))
library(tensorflow)
### Iniciamos um modelo sequencial
mod <- keras_model_sequential()
### Adicionamos as camadas
mod %>%
layer_dense(units              = 2,
activation         = "sigmoid",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0),
input_shape        = ncol(x_treino)) %>%
layer_dense(units              = 1,
activation         = "linear",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0))
library(keras)
model = keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
### Carregamos o pacote keras
library(keras)
library(tensorflow)
### Iniciamos um modelo sequencial
mod <- keras_model_sequential()
### Adicionamos as camadas
mod %>%
layer_dense(units              = 2,
activation         = "sigmoid",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0),
input_shape        = ncol(x_treino)) %>%
layer_dense(units              = 1,
activation         = "linear",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0))
knitr::opts_chunk$set(echo = T, message = F, warning = F)
pacman::p_load("neuralnet", "tidyverse", "latex2exp", "knitr", "NeuralNetTools", "Cairo", "hexbin", "tictoc", "microbenchmark")
### Gerando dados "observados"
set.seed(1.2024)
m.obs <- 100000
dados <- tibble(x1.obs = runif(m.obs, -3, 3),
x2.obs = runif(m.obs, -3, 3)) %>%
mutate(mu = abs(x1.obs^3 - 30*sin(x2.obs) + 10),
y  = rnorm(m.obs, mu, 1))
### Função Sigmoide
sigmoide <- function(x) {
return(1/(1+exp(-x)))
}
### Função Derivada da Sigmoide
derivada_sigmoide <- function(x) {
return(exp(-x)/((1+exp(-x))^2))
}
### Função para calcular o custo
mse_cost <- function(y_true, y_hat) {
return(mean((y_true - y_hat)^2))
}
# Divisão dos dados em treinamento, validação e teste
treino   <- dados[1:80000, ]
val      <- dados[80001:90000, ]
teste    <- dados[90001:nrow(dados), ]
x_treino <- treino %>%
select(x1.obs, x2.obs)
x_val <- val %>%
select(x1.obs, x2.obs)
x_teste  <- teste %>%
select(x1.obs, x2.obs)
y_treino <- treino$y
y_val <- val$y
y_teste  <- teste$y
### Função para fazer o forward propagation com dropout, dados theta, x e p
forward_prop_drop <- function(theta, x, p) {
## Transformação de x para um formato de matriz
ifelse(is.vector(x), x <- as.matrix(x), x <- t(as.matrix(x)))
## Extração dos parâmetros
W1 <- matrix(theta[1:4], 2)
W2 <- matrix(theta[5:6], 2)
b1 <- theta[7:8]
b2 <- theta[9]
## Geração e aplicação das máscaras de cada input
mu_x  <- matrix(rbinom(nrow(x)*ncol(x), 1, p), 2)
x_new <- x * mu_x
## Camada escondida
a <- W1 %*% x_new + matrix(rep(b1, ncol(x)), 2)
h <- sigmoide(a)
## Geração e aplicação das máscaras de cada unidade escondida
mu_h  <- matrix(rbinom(nrow(h)*ncol(h), 1, p), 2)
h_new <- h * mu_h
## Previsão
y_hat <- as.double(t(W2) %*% h_new + b2)
return(y_hat)
}
### Função para realizar o back propagation com dropout, dados theta, x, y e p
back_prop_drop <- function(theta, x, y, p){
## Primeiro, deve-se realizar o forward propagation com dropout
ifelse(is.vector(x), x <- as.matrix(x), x <- t(as.matrix(x)))
W1    <- matrix(theta[1:4], 2)
W2    <- matrix(theta[5:6], 2)
b1    <- theta[7:8]
b2    <- theta[9]
mu_x  <- matrix(rbinom(nrow(x)*ncol(x), 1, p), 2)
x_new <- x * mu_x
a     <- W1 %*% x_new + matrix(rep(b1, ncol(x)), 2)
h     <- sigmoide(a)
mu_h  <- matrix(rbinom(nrow(h)*ncol(h), 1, p), 2)
h_new <- h * mu_h
y_hat <- as.double(t(W2) %*% h_new + b2)
## Em seguida, passamos para a implementação do back propagation
# Camada final: k = 2
g       <- -2*(y - y_hat)/length(y)
grad_b2 <- sum(g)
grad_W2 <- g %*% t(h_new)
g       <- W2 %*% g
# Camada escondida: k = 1
g       <- g * derivada_sigmoide(a) * mu_h
grad_b1 <- rowSums(g)
grad_W1 <- g %*% t(x_new)
## Final: criamos um vetor com os gradientes de cada parâmetro
vetor_grad        <- c(grad_W1, grad_W2, grad_b1, grad_b2)
names(vetor_grad) <- c(paste0("w", 1:6), paste0("b", 1:3))
return(vetor_grad)
}
### Taxa de aprendizagem
epsilon <- 0.1
### Número de iterações
M <- 100
### Probabilidade de inclusão
p <- 0.6
### Lista para receber os parâmetros estimados em cada iteração
theta_est <- list()
### Theta inicial
theta_est[[1]] <- rep(0, 9)
### Vetor para receber o custo de treino em cada iteração
custo_treino <- numeric(M)
### Semente para reproducibilidade
set.seed(1.2024)
### Execução
for(i in 1:M) {
grad             <- back_prop_drop(theta_est[[i]], x_treino, y_treino, p)
custo_treino[i]  <- mse_cost(y_treino, forward_prop_drop(theta_est[[i]], x_treino, p))
theta_est[[i+1]] <- theta_est[[i]] - epsilon*grad
}
min_theta <- unlist(theta_est[which.min(custo_treino)])
round(min_theta, 3)
### Semente para reproducibilidade
set.seed(1.2024)
### Obtenção da população de previsões
A         <- matrix(rep(as.matrix(x_teste[1,]), 200), 200, byrow = T)
y_hat_200 <- forward_prop_drop(min_theta, A, p)
### Semente para reproducibilidade
set.seed(1.2024)
### Função para fazer a previsão de uma observação
pred_c <- function(x, theta, n, p) {
A <- matrix(rep(x, n), n, byrow = T)
return(mean(forward_prop_drop(theta, A, p)))
}
### Cálculo das previsões para todas as observações
y_hat_drop <- apply(x_teste, 1, pred_c, theta = min_theta, n = 200, p = p)
### Apresentação das primeiras 10 previsões
head(y_hat_drop, 10)
### Função para realizar o forward propagation, dados theta e x
forward_prop <- function(theta, x) {
## Transformação de x para um formato de matriz
ifelse(is.vector(x), x <- as.matrix(x), x <- t(as.matrix(x)))
## Extração dos parâmetros
W1 <- matrix(theta[1:4], 2)
W2 <- matrix(theta[5:6], 2)
b1 <- theta[7:8]
b2 <- theta[9]
## Camada escondida
a <- W1 %*% x + matrix(rep(b1, ncol(x)), 2)
h <- sigmoide(a)
## Previsão
y_hat <- as.double(t(W2) %*% h + b2)
return(y_hat)
}
### Estimativas usando weight scaling
theta_scale <- c(min_theta[1:6]*0.6, min_theta[7:9])
y_hat_scale <- forward_prop(theta_scale, x_teste)
### Custo usando o procedimento do item (c)
custo_c <- mse_cost(y_teste, y_hat_drop)
### Custo usando weight scaling
custo_scale <- mse_cost(y_teste, y_hat_scale)
### Comparação do tempo computacional utilizando microbenchmark
microbenchmark('Item c:' = y_hat_drop <- apply(x_teste, 1, pred_c,
theta = min_theta,
n = 200, p = p),
'Scale:'  = y_hat_scale <- forward_prop(theta_scale, x_teste),
times = 5)
### Carregamos o pacote keras
library(keras)
### Iniciamos um modelo sequencial
mod <- keras_model_sequential()
### Iniciamos um modelo sequencial
mod <- keras_model_sequential()
### Adicionamos as camadas
mod %>%
layer_dense(units              = 2,
activation         = "sigmoid",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0),
input_shape        = ncol(x_treino)) %>%
layer_dense(units              = 1,
activation         = "linear",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0))
### Adicionamos as camadas
mod %>%
layer_dense(units              = 2,
activation         = "sigmoid",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0),
input_shape        = ncol(x_treino)) %>%
layer_dense(units              = 1,
activation         = "linear",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0))
### Carregamos o pacote keras
library(keras)
### Iniciamos um modelo sequencial
mod <- keras_model_sequential()
knitr::opts_chunk$set(echo = T, message = F, warning = F)
pacman::p_load("neuralnet", "tidyverse", "latex2exp", "knitr", "NeuralNetTools", "Cairo", "hexbin", "tictoc", "microbenchmark")
### Gerando dados "observados"
set.seed(1.2024)
m.obs <- 100000
dados <- tibble(x1.obs = runif(m.obs, -3, 3),
x2.obs = runif(m.obs, -3, 3)) %>%
mutate(mu = abs(x1.obs^3 - 30*sin(x2.obs) + 10),
y  = rnorm(m.obs, mu, 1))
### Função Sigmoide
sigmoide <- function(x) {
return(1/(1+exp(-x)))
}
### Função Derivada da Sigmoide
derivada_sigmoide <- function(x) {
return(exp(-x)/((1+exp(-x))^2))
}
### Função para calcular o custo
mse_cost <- function(y_true, y_hat) {
return(mean((y_true - y_hat)^2))
}
# Divisão dos dados em treinamento, validação e teste
treino   <- dados[1:80000, ]
val      <- dados[80001:90000, ]
teste    <- dados[90001:nrow(dados), ]
x_treino <- treino %>%
select(x1.obs, x2.obs)
x_val <- val %>%
select(x1.obs, x2.obs)
x_teste  <- teste %>%
select(x1.obs, x2.obs)
y_treino <- treino$y
y_val <- val$y
y_teste  <- teste$y
### Função para fazer o forward propagation com dropout, dados theta, x e p
forward_prop_drop <- function(theta, x, p) {
## Transformação de x para um formato de matriz
ifelse(is.vector(x), x <- as.matrix(x), x <- t(as.matrix(x)))
## Extração dos parâmetros
W1 <- matrix(theta[1:4], 2)
W2 <- matrix(theta[5:6], 2)
b1 <- theta[7:8]
b2 <- theta[9]
## Geração e aplicação das máscaras de cada input
mu_x  <- matrix(rbinom(nrow(x)*ncol(x), 1, p), 2)
x_new <- x * mu_x
## Camada escondida
a <- W1 %*% x_new + matrix(rep(b1, ncol(x)), 2)
h <- sigmoide(a)
## Geração e aplicação das máscaras de cada unidade escondida
mu_h  <- matrix(rbinom(nrow(h)*ncol(h), 1, p), 2)
h_new <- h * mu_h
## Previsão
y_hat <- as.double(t(W2) %*% h_new + b2)
return(y_hat)
}
### Função para realizar o back propagation com dropout, dados theta, x, y e p
back_prop_drop <- function(theta, x, y, p){
## Primeiro, deve-se realizar o forward propagation com dropout
ifelse(is.vector(x), x <- as.matrix(x), x <- t(as.matrix(x)))
W1    <- matrix(theta[1:4], 2)
W2    <- matrix(theta[5:6], 2)
b1    <- theta[7:8]
b2    <- theta[9]
mu_x  <- matrix(rbinom(nrow(x)*ncol(x), 1, p), 2)
x_new <- x * mu_x
a     <- W1 %*% x_new + matrix(rep(b1, ncol(x)), 2)
h     <- sigmoide(a)
mu_h  <- matrix(rbinom(nrow(h)*ncol(h), 1, p), 2)
h_new <- h * mu_h
y_hat <- as.double(t(W2) %*% h_new + b2)
## Em seguida, passamos para a implementação do back propagation
# Camada final: k = 2
g       <- -2*(y - y_hat)/length(y)
grad_b2 <- sum(g)
grad_W2 <- g %*% t(h_new)
g       <- W2 %*% g
# Camada escondida: k = 1
g       <- g * derivada_sigmoide(a) * mu_h
grad_b1 <- rowSums(g)
grad_W1 <- g %*% t(x_new)
## Final: criamos um vetor com os gradientes de cada parâmetro
vetor_grad        <- c(grad_W1, grad_W2, grad_b1, grad_b2)
names(vetor_grad) <- c(paste0("w", 1:6), paste0("b", 1:3))
return(vetor_grad)
}
### Taxa de aprendizagem
epsilon <- 0.1
### Número de iterações
M <- 100
### Probabilidade de inclusão
p <- 0.6
### Lista para receber os parâmetros estimados em cada iteração
theta_est <- list()
### Theta inicial
theta_est[[1]] <- rep(0, 9)
### Vetor para receber o custo de treino em cada iteração
custo_treino <- numeric(M)
### Semente para reproducibilidade
set.seed(1.2024)
### Execução
for(i in 1:M) {
grad             <- back_prop_drop(theta_est[[i]], x_treino, y_treino, p)
custo_treino[i]  <- mse_cost(y_treino, forward_prop_drop(theta_est[[i]], x_treino, p))
theta_est[[i+1]] <- theta_est[[i]] - epsilon*grad
}
min_theta <- unlist(theta_est[which.min(custo_treino)])
round(min_theta, 3)
### Semente para reproducibilidade
set.seed(1.2024)
### Obtenção da população de previsões
A         <- matrix(rep(as.matrix(x_teste[1,]), 200), 200, byrow = T)
y_hat_200 <- forward_prop_drop(min_theta, A, p)
### Semente para reproducibilidade
set.seed(1.2024)
### Função para fazer a previsão de uma observação
pred_c <- function(x, theta, n, p) {
A <- matrix(rep(x, n), n, byrow = T)
return(mean(forward_prop_drop(theta, A, p)))
}
### Cálculo das previsões para todas as observações
y_hat_drop <- apply(x_teste, 1, pred_c, theta = min_theta, n = 200, p = p)
### Apresentação das primeiras 10 previsões
head(y_hat_drop, 10)
### Função para realizar o forward propagation, dados theta e x
forward_prop <- function(theta, x) {
## Transformação de x para um formato de matriz
ifelse(is.vector(x), x <- as.matrix(x), x <- t(as.matrix(x)))
## Extração dos parâmetros
W1 <- matrix(theta[1:4], 2)
W2 <- matrix(theta[5:6], 2)
b1 <- theta[7:8]
b2 <- theta[9]
## Camada escondida
a <- W1 %*% x + matrix(rep(b1, ncol(x)), 2)
h <- sigmoide(a)
## Previsão
y_hat <- as.double(t(W2) %*% h + b2)
return(y_hat)
}
### Estimativas usando weight scaling
theta_scale <- c(min_theta[1:6]*0.6, min_theta[7:9])
y_hat_scale <- forward_prop(theta_scale, x_teste)
### Custo usando o procedimento do item (c)
custo_c <- mse_cost(y_teste, y_hat_drop)
### Custo usando weight scaling
custo_scale <- mse_cost(y_teste, y_hat_scale)
### Comparação do tempo computacional utilizando microbenchmark
microbenchmark('Item c:' = y_hat_drop <- apply(x_teste, 1, pred_c,
theta = min_theta,
n = 200, p = p),
'Scale:'  = y_hat_scale <- forward_prop(theta_scale, x_teste),
times = 5)
### Carregamos o pacote keras
library(keras)
### Iniciamos um modelo sequencial
mod <- keras_model_sequential()
### Iniciamos um modelo sequencial
mod <- keras_model_sequential()
### Adicionamos as camadas
mod %>%
layer_dense(units              = 2,
activation         = "sigmoid",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0),
input_shape        = ncol(x_treino)) %>%
layer_dense(units              = 1,
activation         = "linear",
use_bias           = T,
kernel_initializer = initializer_constant(0),
bias_initializer   = initializer_constant(0))
### Definimos a função de perda e o otimizador
mod %>%
compile(optimizer = optimizer_sgd(lr = 0.1),
loss      = "mse")
### Ajustamos a rede usando o Keras
inicio        <- Sys.time()
mod_resultado <- mod %>%
fit(x               = as.matrix(x_treino),
y               = y_treino,
batch_size      = nrow(x_treino),
epochs          = 100,
callbacks       = list(callback_early_stopping(patience             = 100,
restore_best_weights = T)),
validation_data = list(x_val = as.matrix(x_val), y_val = y_val),
shuffle         = F)
mod
mod_resultado
summary(mod)
setwd("D:/projetos/Consultoria/Carol")
setwd("D:/projetos/consultorias/Carol/")
